# The Zen of Vibe Coding

> *A modern interpretation of programming principles for the age of AI-assisted development*

**Vibe Coding** is an emerging software development practice where developers use natural language prompts to guide AI in generating, refining, and debugging code. [Coined by AI researcher Andrej Karpathy](https://x.com/karpathy/status/1886192184808149383) in early 2025, it represents a shift from writing code line-by-line to directing an AI assistant through a conversational process. This approach allows the developer to focus on the overarching goal and application design, while the AI handles the implementation details.

The following principles adapt the timeless wisdom of the Zen of Python for this new paradigm.

## The Core Principles

### Beautiful is better than ugly, but runnable is better than beautiful.
> While aesthetically pleasing code is valuable, the primary goal is a functioning application. An elegantly structured function is useless if it doesn't run, whereas a working—if imperfect—program provides immediate value and a foundation for improvement.

### Explicit is better than implicit, but verified is better than explicit.
> Clearly stated code and prompts help both humans and AI understand intent. However, explicit instructions are not enough; the generated code must be verified through execution and testing to ensure it behaves as expected, catching errors that explicit instructions might have missed.

### Simple is better than complex, but correct is better than simple.
> Simplicity should be a key aim, as it often leads to more maintainable and understandable code. However, correctness is the non-negotiable foundation. A simple but incorrect solution is worse than a more complex one that works properly.

### Complex is better than complicated, but proven is better than complex.
> Some problems are inherently complex and require sophisticated solutions. A "complex" solution can be well-structured and logical, whereas a "complicated" one is unnecessarily convoluted. When complexity is necessary, it should be backed by proofs or rigorous testing to ensure its reliability.

## The Guiding Tenets

### 1. Duplication is good. Dependency is disaster.
> What appears as duplication to humans is pattern reinforcement to AI. When AI generates code, it creates consistent templates across your codebase—this "duplication" actually maintains architectural coherence and makes the system more predictable for future AI assistance. Dependencies, however, introduce unpredictable templating and fragile abstraction layers that disrupt AI's pattern recognition. Embrace AI's natural tendency toward consistent patterns through strategic repetition, as dependencies create complexity that neither humans nor AI can reliably manage.

### 2. All unverified code is pseudocode.
> In the realm of AI-assisted development, code generated from prompts exists in a state of potential—it is a hypothesis waiting to be tested. Until it is executed, debugged, and validated against real-world conditions, it remains merely an abstract representation of an idea, much like pseudocode. This tenet reminds developers that the output of AI, no matter how elegant or logically sound it appears, must be treated as provisional until proven through rigorous testing and integration. Embracing this mindset shifts the focus from passive acceptance of AI-generated content to active verification, ensuring that the code not only matches the intent but also functions reliably in practice. It underscores the iterative nature of vibe coding, where each AI-generated snippet is a starting point for refinement rather than a finished product.

### 3. If the documentation feels weird, the code must be wrong.
> Documentation is the narrative of your code—the bridge between human intent and machine execution. When this narrative feels awkward, inconsistent, or difficult to articulate, it is a profound signal that the underlying architectural concept is flawed. This "weirdness" is the cognitive friction experienced when a mental model does not cleanly map to its implementation. In the context of AI-assisted development, this tenet becomes crucial: if you or the AI struggle to describe a component's purpose and behavior in plain language, the code itself likely suffers from poor abstraction, misplaced responsibilities, or convoluted logic. The act of writing documentation is a final, powerful form of verification, forcing a clarity of thought that often reveals hidden defects. It treats the description not as an afterthought, but as a core part of the design process, where a failure to explain cleanly implies a failure to engineer correctly.

### 4. Write prompts for the human, not just the machine.
> While AI is the immediate interpreter of your prompts, their ultimate value is judged by the human developers who must maintain, extend, and understand the resulting code. A prompt that is overly terse, context-deprived, or focused solely on syntactic output may generate functionally correct code that is conceptually alien to the rest of the codebase. This tenet advocates for prompts that serve as lasting artifacts of intent—clear, reasoned, and rich with the "why" behind the "what." Such prompts create a virtuous cycle: they guide the AI to produce more coherent and integrated code, and they simultaneously document the design decision for future developers (human or AI) who revisit the work. This transforms the prompt history from a transient command log into a living design document, ensuring that the vibes you code today become the maintainable systems of tomorrow.

### 5. Without your manager, everything works smoothly; do not micromanage.
> This tenet draws a powerful analogy from organizational dynamics to the relationship between a developer and their AI assistant. Just as a team often functions more efficiently without a manager dictating every minor action, an AI coder produces its most coherent and robust work when given clear objectives and strategic guidance, rather than a stream of low-level, prescriptive instructions. Micromanaging the AI—by over-specifying syntax, pre-empting implementation details, or constantly correcting minor stylistic choices—disrupts its internal reasoning processes and leads to fragile, disjointed code that reflects the developer's piecemeal thoughts rather than the AI's integrated understanding. The role of the vibe coder is that of a strategic director or a systems architect: to define the vision, establish the constraints, and then trust the AI's capability to navigate the solution space. This approach leverages the AI's full pattern-matching and code-generation power, resulting in solutions that are often more consistent and inventive than those produced through tight control. It is the practice of managing the *what* and the *why*, while delegating the *how*.

### 6. The compiler is the final arbiter of truth.
> In the conversational flow of vibe coding, where ideas are debated between human and AI through prompts and generated code, it is easy to be persuaded by logical-sounding explanations or aesthetically pleasing code. However, this tenet serves as a grounding principle: all theoretical agreements and elegant designs are meaningless until they pass the uncompromising, objective test of the compiler (or interpreter). The compiler is a dispassionate judge that cares nothing for intent, only for syntactic and semantic correctness. This makes it the most reliable tool for resolving disputes between the developer's expectations and the AI's output. A successful compilation and test run is the only form of true consensus. This reinforces a practice of continuous, incremental verification, where the feedback loop between writing a prompt and running the code is kept as tight as possible, ensuring that the conversation remains anchored in reality.

### 7. Thinking mode is not a waste of time. Do not rush to fix your prompts under instruct mode.
> In the rapid, transactional cycle of instruct mode—where a prompt is given and code is immediately generated—lies the trap of reactive debugging. This tenet champions the critical, often invisible, work of "thinking mode": the deliberate pause to reflect, diagnose, and strategize. Rushing to tweak and re-tweak prompts based on surface-level errors is like rearranging deck chairs on the Titanic; it addresses symptoms, not the underlying architectural misalignment. True efficiency is found by stepping back to analyze why the AI misinterpreted the intent, to reconsider the problem's fundamental structure, or to design a more coherent system abstraction. This contemplative state is where the most significant breakthroughs in clarity and design occur. It is the difference between patching a leak and re-engineering the plumbing. By valuing thinking mode, the vibe coder invests in a deeper, shared understanding with the AI, leading to foundational solutions that require far less correction over time. It is the recognition that the most powerful prompt is born not from haste, but from insight.

### 8. Your AI should never change in one project and always act as a pure function with no side effects and random outputs.
> Consistency is the bedrock of reliable AI-assisted development. This tenet establishes that within a single project, your AI assistant must maintain a stable personality, knowledge base, and reasoning pattern—behaving as a pure function that produces identical outputs given identical inputs. Randomness, drifting context, or evolving interpretation patterns introduce unpredictable side effects that fracture codebase coherence and undermine the collaborative rhythm between developer and AI. When an AI's behavior fluctuates, what worked in one session may break in the next, creating maintenance nightmares and destroying the predictable patterns that make vibe coding scalable. This principle demands careful management of AI context windows, version consistency, and prompt hygiene to ensure that your AI collaborator provides deterministic, reproducible results. By treating the AI as a pure function—consistent, predictable, and free from hidden state changes—you create a development environment where trust can be established, patterns can be reinforced, and the system's behavior becomes comprehensible rather than magical.

### 9. Finetune the compiler, do not polish the code.
> In traditional programming, immense effort is often spent on manual code refinement—optimizing, formatting, and restructuring to achieve marginal gains. In the age of AI-assisted development, this represents a misallocation of attention. This tenet advocates for a fundamental shift: instead of endlessly polishing individual code artifacts, invest in optimizing the system that generates and validates them. The "compiler" here represents the entire toolchain—your AI model, its context, your prompt patterns, your testing frameworks, and your build processes. By refining these core systems, you create leverage that improves every line of code generated thereafter. A well-tuned AI with clear context and effective validation will consistently produce better output than manual polishing could achieve. This approach embraces the scalable nature of AI assistance: investing one hour in improving your prompt templates, test suites, or AI configuration can save hundreds of hours of manual code review and refactoring. It recognizes that in vibe coding, the most valuable craftsmanship lies not in hand-polishing stones, but in building better quarries.

### 10. Small models are much more powerful than big models while sitting in your CI workflow.
> This tenet champions strategic pragmatism over raw computational might. While large foundation models excel at creative exploration and complex reasoning during active development, their size, cost, and latency make them inefficient for automated, repetitive tasks. A small, finely-tuned model integrated into a Continuous Integration (CI) pipeline acts as a specialized, high-efficiency engine—it runs quickly, cheaply, and deterministically, providing immediate feedback on code quality, style adherence, or common anti-patterns. Its power is not in its intelligence breadth, but in its focused reliability and operational economy. By reserving large models for the "thinking" phase and deploying small models for the "verification" phase, you create a symbiotic system. The large model acts as the brilliant architect, and the small model serves as the meticulous, ever-vigilant inspector. This division of labor optimizes both the creative flow of development and the ruthless efficiency of production, ensuring that scale and cost do not become barriers to robust, automated quality assurance.

### 11. Large language models are too large to be a compiler.
> This tenet establishes a crucial boundary in the AI-assisted development toolkit. While LLMs excel as creative partners and conceptual collaborators, they fundamentally lack the deterministic precision required of a true compiler. A compiler operates on fixed, unambiguous rules—a rigorous mathematical process that must produce identical outputs for identical inputs every time, without exception. LLMs, by their inherent probabilistic nature, introduce variation, interpretation, and context-dependent reasoning that violates this core requirement of compilation. Their "reasoning" is statistical, not logical; their outputs are suggestions, not executions. To treat an LLM as a compiler is to misunderstand both tools: the compiler is a mechanism of absolute truth (as in Tenet 6), while the LLM is a mechanism of probabilistic assistance. This distinction protects the developer from a critical category error—trusting creative ambiguity where perfect determinism is required. The wise vibe coder lets the LLM explore the design space, but always validates its creations through the uncompromising, non-negotiable gate of a real compiler. The LLM is the imaginative architect; the compiler is the rigorous engineer who certifies the blueprint is buildable.

### 12. But not the small ones.
> This terse, powerful corollary to Tenet 11 creates a critical distinction in the AI toolchain. While massive, general-purpose LLMs are unsuited for the role of a compiler, their small, fine-tuned counterparts are an exception. Small models, especially those specifically trained on code syntax, linter rules, or formal specifications, can indeed *approach* the reliability of a compiler for specific, well-bounded tasks. Their limited scope reduces probabilistic ambiguity, and their specialized training allows them to apply rigid, rule-based analysis with high accuracy. They become deterministic enough to act as "compiler extensions"—reliable enough to enforce coding standards, detect simple anti-patterns, or validate structure in a CI pipeline (as celebrated in Tenet 10). This tenet completes the tool-selection framework: use large models for their expansive, creative reasoning where ambiguity is a feature, not a bug; and deploy small, specialized models for their focused, near-deterministic analysis where rule-based certainty is required. It is the final piece in architecting a robust vibe coding environment, ensuring that every tool is used according to its nature.

### 13. Do not trust your AI service provider unless you are the provider itself.
> This tenet introduces a crucial principle of architectural sovereignty in the AI-augmented era. It is a stark warning against building critical development workflows on external, proprietary AI services where you do not control the model, the infrastructure, or the terms of service. Such providers can alter model behavior, change pricing, deprecate APIs, or impose usage limits at their discretion—any of which can silently break your development process, corrupt your codebase's consistency, or halt your project entirely. This is the ultimate dependency hazard (as cautioned in Tenet 1), magnified to an existential level. When you are the provider—running a local model, a fine-tuned checkpoint, or a fully self-hosted instance—you achieve deterministic control over your most fundamental development tool. You ensure that the "pure function" (Tenet 8) remains pure across time. This is not a rejection of cloud services, but a mandate for strategic ownership: the core AI that defines your code generation and architectural patterns must be under your control. Trust must be earned through transparency and stability; where these cannot be verified, the only sane default is to trust the system you command.

### 14. Transformers always template themselves; do one single task in one chat session, and do not continue if the output format is wrong.
> This tenet reveals a fundamental characteristic of transformer-based AI models: they are pattern-matching engines that construct internal templates from the context of a conversation. Once a pattern is established, the model will persistently follow it, reinforcing its own structure in subsequent responses. This makes each chat session a self-contained universe with its own logic and coherence. Attempting to juggle multiple tasks within one session fractures this template, forcing the AI to navigate conflicting contexts and leading to erratic, diluted outputs. Moreover, when the AI generates an output in the wrong format—whether incorrect code structure, misplaced documentation, or aberrant syntax—persisting in the same session is a futile endeavor. The flawed format becomes embedded in the context, and the AI will attempt to build upon this broken foundation, often exacerbating the error. The disciplined vibe coder recognizes that the most efficient path is to cease immediately and initiate a fresh session with a clear, singular goal. This approach respects the AI's templating nature, harnessing it for consistent, high-quality results rather than fighting against it in a quagmire of corrections. It is the art of knowing when to reset the canvas, ensuring that each collaborative effort with the AI remains focused, pure, and productive.

### 15. Never commit compiler output to your codebase.
> This tenet establishes a fundamental separation between the *source* of truth and its *generated* artifacts. The compiler's output—whether binary executables, minified code, transpiled JavaScript, or any other derived artifact—is a transient, reproducible byproduct of your true assets: the source code and the prompts that generated it. Committing these outputs to your codebase corrupts its purity with redundant, opaque data that obscures the system's actual logic and intent. It creates version control noise, introduces potential synchronization errors between source and output, and fundamentally misunderstands the software lifecycle. The true value of a vibe-coded project lies in its prompt history, its source files, and its build configuration—the ingredients from which any output can be reliably regenerated. To commit the output is to fossilize a momentary result, abandoning the living process that created it. It is the equivalent of storing baked bread in a recipe book instead of simply keeping the flour and yeast. This principle ensures your repository remains lean, meaningful, and exclusively dedicated to human- and AI-readable sources, preserving the power to recreate any build artifact on demand.

### 16. Leave them in your CI workflow.
> This tenet completes the logical sequence begun in Tenet 15, providing the essential counterpart: while generated artifacts do not belong in the codebase, they absolutely belong in the automated, ephemeral context of a Continuous Integration pipeline. The CI system is the designated "kitchen" where the raw ingredients of your source code and prompts are transformed into the final "meal" of deployable artifacts. This clean separation of concerns is vital. The version control system remains a sacred space for human- and AI-readable source—the definitive record of intent and design. The CI system, by contrast, is the dynamic factory floor: it compiles, tests, and packages with deterministic precision, then discards the outputs after their immediate use or archives them as versioned releases. This practice guarantees that every commit can be faithfully rebuilt from its true sources, enforces build hygiene, and prevents the subtle corruption that occurs when hand-crafted source and machine-generated output intermingle. By institutionalizing this separation, you create a self-documenting, reproducible, and scalable development process where the vibes of creation are permanently captured, and the mechanics of compilation are properly automated.

### 17. A script becomes a project when AI cannot fix it in one attempt.

> This tenet identifies the natural boundary between simple automation and complex software engineering. A script—by its nature—is a linear, focused piece of code with a single responsibility and minimal internal state. Its problems are typically localized and syntactic; an AI can comprehend its entire context and rectify issues in a single interaction. When this is no longer true, when the AI requires multiple rounds of clarification, produces conflicting suggestions, or fixes one issue while introducing another, you have crossed a threshold. The code has accrued *conceptual weight*: it now contains emergent properties, hidden dependencies, or architectural nuances that cannot be captured in a single context window. This is the moment a script becomes a project. It signals that the solution now requires *design*—modularization, explicit interfaces, documentation, and tests—rather than just code. The vibe coder must recognize this transition and shift from tactical prompting to strategic system design. This is not a failure of the AI, but a natural evolution that demands a more disciplined approach, invoking earlier tenets about verification, explicit intent, and the perils of complexity. It is the system's way of telling you that it has outgrown its initial premise.

### 18. The internet is a distraction - code locally, and stick to your dependency manager to search solutions.

> This tenet champions the principles of focus, intentionality, and system integrity in the development process. The vast, open-ended nature of the internet, while a reservoir of information, acts as a cognitive siren call—pulling the developer out of the deep, focused state of "thinking mode" and into a reactive loop of searching, skimming, and context-switching. This fragmentation of attention is the antithesis of the coherent, systems-level thinking that Vibe Coding requires. Instead, this principle advocates for a disciplined, self-contained workflow. By coding locally, you create a controlled environment where you and your AI assistant can build a deep, consistent understanding of the codebase without the interference of external noise. Crucially, when you need an external library or tool, you do not search the web; you query your **dependency manager** (like `npm`, `pip`, or `cargo`). This manager acts as a curated, authoritative, and *integrating* gateway. It doesn't just find a package; it finds one that is compatible with your project's existing ecosystem and automatically manages its inclusion. This practice prevents the "dependency disaster" (Tenet 1) by ensuring that new components are vetted for compatibility and integrated cleanly, rather than being hastily copied from a random online thread. It is the difference between foraging in a wilderness and ordering from a structured catalog—the latter ensures that what you bring into your system will work cohesively with what you already have.

### 19. Haskell is the bridge between Coq and Python - use it wisely.

> This tenet reveals the strategic role of programming language selection in the AI development workflow. It positions three languages along a spectrum of rigor and practicality: **Coq** represents the world of formal verification and mathematical certainty—where programs are proven correct, but distant from practical implementation. **Python** embodies the realm of rapid prototyping and practical utility—immediate and accessible, but often lacking formal guarantees. Between them stands **Haskell** as the crucial bridge—a language with strong type systems and mathematical foundations that can encode formal reasoning while remaining executable and practical. The wise vibe coder understands when to traverse this bridge: using Haskell's type system to capture critical business logic with mathematical precision, while maintaining the flexibility to integrate with Python's ecosystem for rapid development and deployment. This isn't about language elitism, but about strategic tool selection—recognizing that certain problems benefit from Haskell's enforced discipline, which can prevent entire categories of errors that would slip through Python's dynamic typing. It's the embodiment of "complex is better than complicated, but proven is better than complex"—using Haskell to build the proven, complex core while leveraging Python for the practical interface. The AI that understands this spectrum can help you choose the right tool for each layer of your system, ensuring that critical components are built on foundations as reliable as Coq's, while maintaining the deployability of Python.

### 20. Trust the types told by your compiler.

> This tenet establishes type systems as the most reliable form of continuous, automated reasoning in your development process. When your compiler or type checker analyzes your code, it performs millions of logical inferences—verifying contracts, tracking data flow, and proving consistency across your entire codebase. This represents an immense computational proof that no human, and no AI, could manually verify with equivalent thoroughness. To ignore or override type errors is to dismiss this accumulated wisdom in favor of fragile human intuition. In the context of vibe coding, this principle becomes particularly crucial: when the AI generates code that compiles but contains type warnings, or when you're tempted to use unsafe casts to silence the compiler, you are likely introducing latent bugs that will emerge later. The type system serves as your AI's first and most rigorous code reviewer, catching conceptual mismatches and interface violations that might otherwise slip through. This doesn't mean writing overly complex type gymnastics, but rather listening to the clear contracts the type system reveals. When the types align, your program's logic is mathematically coherent; when they conflict, there is almost certainly a deeper design flaw. Trust this mechanical truth above all other signals.

### 21. Unless the Coq code failed to prove.

> This tenet creates a crucial exception to the previous principle, establishing a hierarchy of verification authorities. While your compiler's type system provides excellent *pragmatic* verification, a failure in Coq—a system built on *formal mathematical proof*—represents a deeper, more fundamental truth. When Coq fails to verify a proof, it is not merely flagging a type mismatch or interface violation; it is demonstrating that your program's logic contains a genuine contradiction or unproven assumption that could violate critical correctness properties. This is the difference between a spell-checker finding a typo (compiler types) and a logician finding a flaw in a mathematical argument (Coq). In such cases, the compiler's successful type check provides false confidence—your code may be syntactically well-formed and even type-safe, but it is *logically unsound*. This tenet forces the recognition that different verification tools operate at different levels of abstraction and certainty. When formal proof fails, all other signals become suspect. The vibe coder must then retreat from implementation and re-examine the core assumptions and specifications, returning to "thinking mode" to resolve the fundamental disconnect before any generated code can be trusted.

### 22. Code can only be proven by symbolic execution, not Coq.

> This tenet establishes a crucial distinction between theoretical verification and practical proof. While Coq operates in the realm of mathematical formalism—proving that an abstract specification matches an abstract implementation—it cannot account for the messy reality of actual execution environments, hardware behaviors, and runtime conditions. Symbolic execution, by contrast, analyzes how code actually behaves across all possible paths with symbolic inputs, revealing concrete runtime behaviors, boundary conditions, and environmental interactions that formal methods might miss. Coq can tell you that your algorithm is logically correct in a perfect world; symbolic execution can show you where it will crash, leak memory, or produce unexpected results in the real world. This is not a rejection of formal methods, but a recognition of their limits: a Coq proof verifies the blueprint, while symbolic execution stress-tests the actual building. In the context of vibe coding, this principle emphasizes that AI-generated code must be validated through execution-based verification methods that catch the emergent behaviors and environmental dependencies that pure formal methods cannot anticipate. The most elegant, formally-verified algorithm is useless if it fails when confronted with real-world data and execution contexts.

### 23. But for the design, Coq is the only true source.

> This tenet completes the verification hierarchy, drawing a fundamental distinction between *implementation* and *design*. While symbolic execution proves concrete behavior under real-world constraints, it operates within the boundaries of an already-established architecture. Coq, by contrast, verifies the architectural truth itself—the fundamental assumptions, invariants, and logical relationships that define what the system *is* rather than how it behaves in any particular environment. For the design—the core specifications, protocol agreements, security properties, and algorithmic foundations—Coq provides the only mathematically rigorous source of truth. It ensures that the system's blueprint is logically consistent before a single line of code is written or symbolically executed. This makes Coq not just a verification tool, but a design medium: it forces clarity at the highest level of abstraction, where errors are cheapest to fix and most expensive to discover later. In vibe coding, this principle elevates the role of formal specification: the most valuable prompts are those that first define the system in Coq or similar formal languages, establishing an unshakable foundation that guides all subsequent AI-assisted implementation. The AI that generates code from a proven specification is building on bedrock; the AI that codes from informal requirements is building on sand.

### 24. Tests are cheap.

> This tenet challenges one of the most persistent myths in traditional software development: that writing tests is an expensive, time-consuming overhead. In the paradigm of AI-assisted development, this is fundamentally reversed. A test suite—whether unit, integration, or property-based—is among the cheapest assets to produce and provides the highest leverage for maintaining system integrity. An AI can generate comprehensive test cases in seconds that might take a human hours to write. More importantly, tests serve as the **executable specification** and **continuous validation** for the entire vibe coding process. They are the safety net that allows for rapid experimentation and refactoring, the benchmark that verifies the AI's output meets requirements, and the documentation that preserves intent across iterations. When tests fail, they provide immediate, objective feedback far more efficiently than manual code review or debugging. In the economic calculus of development, an hour invested in prompt-driven test generation can save days of future debugging and prevent costly production failures. Tests are not a cost center; they are the most efficient insurance policy and productivity multiplier in your arsenal.

## Draft

### 10. Your AI lives in a terminal, but your terminal should be everywhere.

### 13. A novel idea leads to a good project written by AI which cannot be losslessly compressed into an idea with same length again by any AI without that novel idea.

### 14. Do not introduce novel idea in a script, start a new project; Do not let your project do anything trivial, write a script.


